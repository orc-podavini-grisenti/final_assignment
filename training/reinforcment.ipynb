{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "nhpYDINPRKzp",
      "metadata": {
        "id": "nhpYDINPRKzp"
      },
      "source": [
        "### Cell 0 â€“ Repository Setup (Colab Only)\n",
        "\n",
        "**Purpose**: This cell exists only for Google Colab execution. It ensures the correct repository and branch are downloaded before running the experiment.\n",
        "\n",
        "**What Happens**:\n",
        "- Clones the GitHub repository\n",
        "- Selects the trajectory_tracking_policy branch\n",
        "- Sets the project root so Python can resolve imports correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cMzorY91RWhY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMzorY91RWhY",
        "outputId": "ecba5a35-947f-4ae6-9c54-53d27d34c6c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'final_assignment' already exists and is not an empty directory.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# 1. Clone the repository\n",
        "!git clone -b trajectory_tracking_policy https://github.com/orc-podavini-grisenti/final_assignment\n",
        "\n",
        "# 2. Point the project_root specifically to the cloned folder\n",
        "# In Colab, the folder structure will be /content/final_assignment/\n",
        "project_root = os.path.abspath('/content/final_assignment')\n",
        "\n",
        "# 3. Add that specific folder to the system path\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "\n",
        "\n",
        "# 4. Link the Drive to the colab in order to store the results\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_path = \"/content/drive/MyDrive/final_assignment/outputs/models_saved\"\n",
        "\n",
        "# Crea la cartella se non esiste\n",
        "if not os.path.exists(drive_path):\n",
        "    os.makedirs(drive_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12184240",
      "metadata": {
        "id": "12184240"
      },
      "source": [
        "### Cell 1: Imports & Global Setup\n",
        "This cell initializes all core dependencies and defines shared configuration variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "efb0bce8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efb0bce8",
        "outputId": "7dbc3dcc-84ee-4afa-c5dc-760fde739393"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on: cuda\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- PATH SETUP ---\n",
        "# Add the project root to sys.path so we can import envs/planner\n",
        "# Adjust '..' depending on where this notebook is located relative to root\n",
        "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "\n",
        "from envs.unicycle_env import UnicycleEnv\n",
        "from planner.dubins_planner import DubinsPlanner\n",
        "from models.trajectory_tracking_network import TTNetwork\n",
        "from models.value_network import ValueNetwork\n",
        "from utils.normalization import ObservationNormalizer\n",
        "from utils.reward import TrajectoryTrackingReward\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Running on: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9f474e8",
      "metadata": {
        "id": "f9f474e8"
      },
      "source": [
        "### Cell 2: Experiment Configuration \n",
        "Defines hyperparameters and experiment-level settings; plus intialize the directory for all the output of the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "726a095a",
      "metadata": {
        "id": "726a095a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results will be saved to: experiments/run_20260109_100247\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "MAX_EPISODES = 4000\n",
        "MAX_STEPS    = 200\n",
        "POLICY_LR     = 1e-3\n",
        "BATCH_SIZE   = 32\n",
        "GAMMA        = 0.99\n",
        "\n",
        "SEED         = 0        # Seed for reproducibility\n",
        "PATIENCE     = 10       # Stop after 10 updates (200 episodes) without improvement\n",
        "\n",
        "EXPERIMENTS_PATH = drive_path if 'drive_path' in locals() else \"experiments/\"\n",
        "\n",
        "\n",
        "# Global Output Directory\n",
        "run_id = f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "output_dir = os.path.join(EXPERIMENTS_PATH, run_id)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"Results will be saved to: {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "655cb984",
      "metadata": {
        "id": "655cb984"
      },
      "source": [
        "### Cell 3 â€“ Environment Initialization\n",
        "Creates the reinforcement learning environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c4967fd",
      "metadata": {
        "id": "7c4967fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed set to: 0\n"
          ]
        }
      ],
      "source": [
        "# MENAGE THE SEED for REPRODUCIBILITY \n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "print(f\"Seed set to: {SEED}\")\n",
        "\n",
        "\n",
        "# INIT THE ENVIROMENT \n",
        "env = UnicycleEnv('empty_env')\n",
        "\n",
        "# INIT THE REWARD CALCULATOR  \n",
        "reward_calculator = TrajectoryTrackingReward()\n",
        "\n",
        "\n",
        "# HELPER\n",
        "def sample_radius(mean=1.65, std=0.3, min_r=1.0, max_r=2.5):\n",
        "    \"\"\"Samples a radius from a truncated Gaussian distribution.\"\"\"\n",
        "    radius = np.random.normal(loc=mean, scale=std)\n",
        "    return np.clip(radius, min_r, max_r)\n",
        "\n",
        "def sample_seed(min_eval_seed=100):\n",
        "    \"\"\"Samples a seed, skipping the first 100 reserved for evaluation.\"\"\"\n",
        "    return random.randint(min_eval_seed, 1000000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "110b4271",
      "metadata": {},
      "source": [
        "### 4. Networks Initialization\n",
        "\n",
        "**TTNetwork (Trajectory Tracking Network):** This is our **Policy Network (Actor)**. It learns the mapping from states to actions, determining the best step to take at any given time. It is a simple multi layer netweork. With the network we have also:\n",
        "\n",
        "* Adam Optimizer + a scheduler to reduce the learning rate\n",
        "* Normalizer of the Observable, which normalize the input of the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "a06b47e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# POLICY NETWORK\n",
        "OBS_DIM = 3     # Obs dim is 3 (rho, alpha, d_theta)\n",
        "ACTION_DIM = 2  # v, omega\n",
        "\n",
        "policy = TTNetwork(OBS_DIM, ACTION_DIM).to(device)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=POLICY_LR)\n",
        "# This helps the network \"settle\" into the precise solution\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.5) # Reduces LR by factor of 0.5 every 200 episodes\n",
        "\n",
        "# Normalizer for the Policy Input\n",
        "obs_normalizer = ObservationNormalizer(max_dist=3.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afd6c402",
      "metadata": {},
      "source": [
        "### Cell 5: Data Managmement\n",
        "\n",
        "The TrajectoryBuffer class acts as the central repository for all agent experiences accumulated during the training. In this way we have a clean and structured data management\n",
        "\n",
        "**Key Logic**: Returns-to-GoThe finish_episode method implements the discounted return calculation ($G_t$). This converts immediate rewards into a long-term signal, which is essential for the Policy Gradient update:\n",
        "$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "5c82a23f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrajectoryBuffer:\n",
        "    def __init__(self):\n",
        "        # --- 1. Current Episode Storage ---\n",
        "        self.episode_log_probs = [] \n",
        "        self.episode_rewards = []\n",
        "        \n",
        "        # --- 2. Batch Storage (Accumulated for the Update) ---\n",
        "        self.batch_log_probs = []   \n",
        "        self.batch_returns = []            \n",
        "\n",
        "        # --- 3. Global History Storage (For Plotting) ---\n",
        "        self.episode_reward_history = []    # Total reward per episode\n",
        "        self.batch_reward_history = []      # Avg reward per batch\n",
        "        self.policy_loss_history = []       # Policy loss per batch update\n",
        "\n",
        "    def store_step(self, log_prob):\n",
        "        self.episode_log_probs.append(log_prob)\n",
        "\n",
        "    def store_reward(self, reward):\n",
        "        self.episode_rewards.append(reward)\n",
        "\n",
        "    def store_metrics(self, episode_reward=None, batch_reward=None, policy_loss=None):\n",
        "        \"\"\"\n",
        "        Logs metrics to history. \n",
        "        - episode_reward is usually logged every episode.\n",
        "        - batch_reward, policy_loss and value_loss are usually logged every batch update.\n",
        "        \"\"\"\n",
        "        if episode_reward is not None:\n",
        "            self.episode_reward_history.append(episode_reward)\n",
        "        \n",
        "        if batch_reward is not None:\n",
        "            self.batch_reward_history.append(batch_reward)\n",
        "\n",
        "        if policy_loss is not None:\n",
        "            self.policy_loss_history.append(policy_loss)\n",
        "            \n",
        "\n",
        "    def finish_episode(self, gamma):\n",
        "        \"\"\"Calculates Returns-to-Go and moves data to batch storage.\"\"\"\n",
        "        G_t = 0\n",
        "        returns = []\n",
        "        for r in reversed(self.episode_rewards):\n",
        "            G_t = r + (gamma * G_t)\n",
        "            returns.insert(0, G_t)\n",
        "        \n",
        "        self.batch_log_probs.extend(self.episode_log_probs)\n",
        "        self.batch_returns.extend(returns)\n",
        "        \n",
        "        # Clear episode memory\n",
        "        self.episode_log_probs = []\n",
        "        self.episode_rewards = []\n",
        "\n",
        "    def clear_batch(self):\n",
        "        self.batch_log_probs = []\n",
        "        self.batch_returns = []\n",
        "\n",
        "# Usage\n",
        "buffer = TrajectoryBuffer()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a445984",
      "metadata": {
        "id": "5a445984"
      },
      "source": [
        "### 6. Helper Functions\n",
        "\n",
        "ğŸ“ Path Tracking & State Preparation\n",
        "The primary challenge is transforming a \"reach-the-goal\" environment into a \"follow-the-path\" task.\n",
        "\n",
        "* **`get_tracking_obs`**\n",
        "    The environment's default observation provides data relative to a final static goal. This function recalculates the state so the agent observes its position relative to the **next waypoint** identified above:\n",
        "    * **$\\rho$**: Distance to the current waypoint.\n",
        "    * **$\\alpha$**: Angle to the waypoint relative to the agent's heading.\n",
        "    * **$\\Delta\\theta$**: Orientation error relative to the desired path direction.\n",
        "\n",
        "* **`get_path_tracking_info`**\n",
        "    Finding the next waypoint is non-trivial because the agentâ€”especially during early trainingâ€”may overshoot or miss a point. A simple incremental index is not robust enough. \n",
        "    * **Logic:** We identify the waypoint **closest** to the agent's current position and then target the **waypoint immediately following it**. \n",
        "\n",
        "\n",
        "ğŸ® Action Selection\n",
        "\n",
        "* **`get_action`**\n",
        "    This function interfaces with the **TTNetwork** to determine movement.Rather than choosing the single \"best\" action, we sample from the policy's probability distribution. This randomness is crucial during training; it ensures the agent explores different trajectories to find the most efficient path rather than getting stuck in a local optimum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "86545a72",
      "metadata": {
        "id": "86545a72"
      },
      "outputs": [],
      "source": [
        "def normalize_angle(angle):\n",
        "    \"\"\"Wraps an angle to the range [-pi, pi].\"\"\"\n",
        "    return (angle + np.pi) % (2 * np.pi) - np.pi\n",
        "\n",
        "   \n",
        "def get_tracking_obs(target_waypoint, robot_state):\n",
        "    \"\"\"\n",
        "    Computes rho (distance), alpha (angle error), and d_theta (heading error).\n",
        "    \n",
        "    Args:\n",
        "        target_waypoint: [x, y, theta] of target\n",
        "        robot_state: [x, y, theta] of current robot position\n",
        "    \"\"\"\n",
        "    # 1. Compute position errors\n",
        "    dx = target_waypoint[0] - robot_state[0]\n",
        "    dy = target_waypoint[1] - robot_state[1]\n",
        "    \n",
        "    # Rho: Euclidean distance\n",
        "    rho = np.sqrt(dx**2 + dy**2)\n",
        "    \n",
        "    # Alpha: Angle to waypoint relative to robot heading\n",
        "    # Formula: atan2(dy, dx) - current_theta\n",
        "    alpha = normalize_angle(np.arctan2(dy, dx) - robot_state[2])\n",
        "    \n",
        "    # d_theta: Heading alignment error\n",
        "    d_theta = normalize_angle(target_waypoint[2] - robot_state[2])\n",
        "\n",
        "    return np.array([rho, alpha, d_theta], dtype=np.float32)\n",
        "\n",
        "\n",
        "def get_path_tracking_info(path, robot_state, current_path_idx):\n",
        "    \"\"\"\n",
        "    Calculates tracking error and determines progress along the path.\n",
        "    \n",
        "    Returns:\n",
        "        error (float): Distance to the closest point on the path.\n",
        "        next_idx (int): The updated waypoint index the robot should target.\n",
        "        checkpoints_cleared (int): Number of new waypoints passed in this step.\n",
        "    \"\"\"\n",
        "    # 1. Extract (x, y) from the path\n",
        "    path_xy = path[:, :2]\n",
        "    \n",
        "    # 2. Compute distances to all waypoints to find the closest one\n",
        "    dists = np.linalg.norm(path_xy - robot_state[:2], axis=1)\n",
        "    error = np.min(dists)\n",
        "    closest_idx = np.argmin(dists)\n",
        "    \n",
        "    # 3. Lookahead Logic:\n",
        "    # We target the waypoint immediately following the closest one.\n",
        "    # We use max() to ensure the robot never \"goes back\" in its target index.\n",
        "    next_idx = max(current_path_idx, closest_idx + 1)\n",
        "    next_idx = min(next_idx, len(path) - 1)\n",
        "    \n",
        "    # 4. Progress Calculation:\n",
        "    # This is the \"Checkpoints Cleared\" logic. \n",
        "    # It counts how many waypoints we advanced beyond the previous target.\n",
        "    checkpoints_cleared = next_idx - current_path_idx\n",
        "    checkpoints_cleared = max(0, checkpoints_cleared)\n",
        "    \n",
        "    return error, next_idx, checkpoints_cleared\n",
        "\n",
        "\n",
        "def get_action(trajectory_obs):\n",
        "    \"\"\"\n",
        "    Processes observation, samples action, and records transition data.\n",
        "    \"\"\"\n",
        "    # 1. Pre-process and move to device\n",
        "    # Use the imported normalizer to normaliza the inputs features to give the same relevance\n",
        "    norm_obs = obs_normalizer.normalize_tt(trajectory_obs)\n",
        "    obs_t = torch.as_tensor(norm_obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "    # 2. Policy Forward Pass\n",
        "    mean, std = policy(obs_t)\n",
        "    dist = Normal(mean, std)\n",
        "    \n",
        "    # 3. Sample Action (stochastic for exploration)\n",
        "    action = dist.sample()\n",
        "    log_prob = dist.log_prob(action).sum(dim=1)\n",
        "\n",
        "    # 4. Store transition data in the buffer \n",
        "    buffer.store_step(log_prob)\n",
        "\n",
        "    # 5. Convert to numpy and clip for the environment\n",
        "    action_np = action.cpu().detach().numpy()[0]\n",
        "    return np.clip(action_np, -1.0, 1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5a65dfe",
      "metadata": {
        "id": "d5a65dfe"
      },
      "source": [
        "### 7. REINFORCE Update\n",
        "This cell implements the training step for the REINFORCE algorithm.\n",
        "\n",
        "ğŸ”„ The Update Process\n",
        "\n",
        "1.  **Prepare Batch Data:** Convert cumulative returns ($G_t$) and observations into tensors, ensuring shapes are compatible for vectorized processing.\n",
        "2.  **Calculate Policy Loss:** * Compute the loss using the formula: $L = -\\log(\\pi(a_t | s_t)) \\cdot G_t$.\n",
        "    * Positive advantage increases action probability; negative advantage decreases it.\n",
        "3.  **Update Policy Network:** * Apply gradient clipping to prevent exploding gradients and update weights via backpropagation.\n",
        "4.  **Memory Management:** Clear batch buffers (log_probs, returns, observations) to prevent memory leaks and ensure the next batch starts fresh."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "b58828e9",
      "metadata": {
        "id": "b58828e9"
      },
      "outputs": [],
      "source": [
        "def update():\n",
        "   \n",
        "    # 0. Check if we have enough data\n",
        "    if len(buffer.batch_returns) == 0:\n",
        "        return\n",
        "\n",
        "    # 1. PREPARE BATCH DATA:\n",
        "    # Use torch.stack on the existing log_prob tensors to maintain the grad_fn\n",
        "    log_probs_t = torch.stack(buffer.batch_log_probs).view(-1, 1)\n",
        "    # Returns (G_t) are constants, so these don't need gradients\n",
        "    returns_t = torch.tensor(buffer.batch_returns).float().to(device).view(-1, 1)\n",
        "    \n",
        "    # Apply Batch-Wise Normalization to G_t to stabilize the scale of policy updates.\n",
        "    if len(returns_t) > 1:\n",
        "        returns_t = (returns_t - returns_t.mean()) / (returns_t.std() + 1e-8)\n",
        "\n",
        "       \n",
        "    # 2. CALCULATE POLICY LOSS:\n",
        "    # Iterate through sampled actions to calculate: \n",
        "    #   Policy Loss = 1/N sum_N ( -log(Ï€(u_t | x_t)) * A_t )\n",
        "    policy_loss = -(log_probs_t * returns_t).mean()\n",
        "    p_loss_val = policy_loss.item()\n",
        "\n",
        "\n",
        "    # 3. UPDATE POLICY NETWORK\n",
        "    # Perform backpropagation to calculate policy gradients.\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    # Apply gradient clipping and update policy parameters.\n",
        "    torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    # 4. CLEANUP & MEMORY MANAGEMENT:\n",
        "    # Store metrics and wipe the batch buffer for the next training iteration\n",
        "    buffer.store_metrics(policy_loss=p_loss_val)\n",
        "    buffer.clear_batch()\n",
        "\n",
        "\n",
        "    return p_loss_val"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07f76da5",
      "metadata": {
        "id": "07f76da5"
      },
      "source": [
        "### Cell 8: Training\n",
        "\n",
        "This cell executes the main training cycle. The process is divided into environment setup, agent interaction, and batch-wise policy updates.\n",
        "\n",
        "ğŸ‹ï¸ The Training Workflow\n",
        "\n",
        "1.  **Episode Setup:** For each episode, we sample a new random seed and plan a smooth trajectory using a **Dubins Path Planner** with varying curvatures.\n",
        "2.  **Interaction Loop (The Episode):**\n",
        "    * **State Transformation:** The raw environment state is converted into an **Ego-Centric Observation** $(\\rho, \\alpha, \\delta\\theta)$ relative to the next waypoint.\n",
        "    * **Action & Environment Step:** The agent samples an action from the policy, moves, and receives a reward based on tracking accuracy and terminal status.\n",
        "    * **Terminal Conditions:** Episodes end if the agent reaches the goal, collides, wanders too far off-path (`error > 2.0`), or times out.\n",
        "3.  **Batch Learning:** Every `BATCH_SIZE` episodes, the `update()` function is called to perform backpropagation on both the Policy and Value networks.\n",
        "4.  **Early Stopping & Checkpointing:**\n",
        "    * The model monitors the **Average Reward** of the last batch.\n",
        "    * If a new best performance is achieved, the model weights are saved to disk.\n",
        "    * If performance plateaus for a defined period (`PATIENCE`), training terminates early to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "196fb4c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "196fb4c8",
        "outputId": "9fa17839-f78e-4d89-ab54-93e6b3ffc8f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Training...\n",
            "Update performed at episode 31\n",
            "Update 0 | New Best: 22.17 | Model Saved.\n",
            "Update performed at episode 63\n",
            "Update 1 | New Best: 63.46 | Model Saved.\n",
            "Update performed at episode 95\n",
            "Update 2 | Avg Reward: 27.72 | Patience: 1/10\n",
            "Update performed at episode 127\n",
            "Update 3 | Avg Reward: 50.77 | Patience: 2/10\n",
            "Update performed at episode 159\n",
            "Update 4 | New Best: 73.57 | Model Saved.\n",
            "Update performed at episode 191\n",
            "Update 5 | New Best: 140.06 | Model Saved.\n",
            "Update performed at episode 223\n",
            "Update 6 | Avg Reward: 69.86 | Patience: 1/10\n",
            "Update performed at episode 255\n",
            "Update 7 | Avg Reward: 128.30 | Patience: 2/10\n",
            "Update performed at episode 287\n",
            "Update 8 | New Best: 158.82 | Model Saved.\n",
            "Update performed at episode 319\n",
            "Update 9 | New Best: 192.74 | Model Saved.\n",
            "Update performed at episode 351\n",
            "Update 10 | New Best: 196.95 | Model Saved.\n",
            "Update performed at episode 383\n",
            "Update 11 | New Best: 233.16 | Model Saved.\n",
            "Update performed at episode 415\n",
            "Update 12 | Avg Reward: 229.80 | Patience: 1/10\n",
            "Update performed at episode 447\n",
            "Update 13 | Avg Reward: 232.58 | Patience: 2/10\n",
            "Update performed at episode 479\n",
            "Update 14 | New Best: 271.68 | Model Saved.\n",
            "Update performed at episode 511\n",
            "Update 15 | Avg Reward: 220.77 | Patience: 1/10\n",
            "Update performed at episode 543\n",
            "Update 16 | New Best: 292.31 | Model Saved.\n",
            "Update performed at episode 575\n",
            "Update 17 | Avg Reward: 260.69 | Patience: 1/10\n",
            "Update performed at episode 607\n",
            "Update 18 | New Best: 323.05 | Model Saved.\n",
            "Update performed at episode 639\n",
            "Update 19 | Avg Reward: 250.83 | Patience: 1/10\n",
            "Update performed at episode 671\n",
            "Update 20 | Avg Reward: 314.69 | Patience: 2/10\n",
            "Update performed at episode 703\n",
            "Update 21 | Avg Reward: 293.94 | Patience: 3/10\n",
            "Update performed at episode 735\n",
            "Update 22 | Avg Reward: 285.63 | Patience: 4/10\n",
            "Update performed at episode 767\n",
            "Update 23 | Avg Reward: 312.57 | Patience: 5/10\n",
            "Update performed at episode 799\n",
            "Update 24 | Avg Reward: 299.45 | Patience: 6/10\n",
            "Update performed at episode 831\n",
            "Update 25 | Avg Reward: 297.26 | Patience: 7/10\n",
            "Update performed at episode 863\n",
            "Update 26 | New Best: 325.42 | Model Saved.\n",
            "Update performed at episode 895\n",
            "Update 27 | Avg Reward: 298.72 | Patience: 1/10\n",
            "Update performed at episode 927\n",
            "Update 28 | Avg Reward: 310.34 | Patience: 2/10\n",
            "Update performed at episode 959\n",
            "Update 29 | Avg Reward: 234.14 | Patience: 3/10\n",
            "Update performed at episode 991\n",
            "Update 30 | Avg Reward: 287.03 | Patience: 4/10\n",
            "Update performed at episode 1023\n",
            "Update 31 | Avg Reward: 284.49 | Patience: 5/10\n",
            "Update performed at episode 1055\n",
            "Update 32 | Avg Reward: 275.15 | Patience: 6/10\n",
            "Update performed at episode 1087\n",
            "Update 33 | Avg Reward: 269.54 | Patience: 7/10\n",
            "Update performed at episode 1119\n",
            "Update 34 | New Best: 326.42 | Model Saved.\n",
            "Update performed at episode 1151\n",
            "Update 35 | New Best: 345.60 | Model Saved.\n",
            "Update performed at episode 1183\n",
            "Update 36 | Avg Reward: 296.04 | Patience: 1/10\n",
            "Update performed at episode 1215\n",
            "Update 37 | Avg Reward: 300.24 | Patience: 2/10\n",
            "Update performed at episode 1247\n",
            "Update 38 | Avg Reward: 319.70 | Patience: 3/10\n",
            "Update performed at episode 1279\n",
            "Update 39 | Avg Reward: 315.33 | Patience: 4/10\n",
            "Update performed at episode 1311\n",
            "Update 40 | Avg Reward: 304.48 | Patience: 5/10\n",
            "Update performed at episode 1343\n",
            "Update 41 | New Best: 353.48 | Model Saved.\n",
            "Update performed at episode 1375\n",
            "Update 42 | Avg Reward: 346.38 | Patience: 1/10\n",
            "Update performed at episode 1407\n",
            "Update 43 | Avg Reward: 319.19 | Patience: 2/10\n",
            "Update performed at episode 1439\n",
            "Update 44 | Avg Reward: 342.82 | Patience: 3/10\n",
            "Update performed at episode 1471\n",
            "Update 45 | Avg Reward: 265.83 | Patience: 4/10\n",
            "Update performed at episode 1503\n",
            "Update 46 | Avg Reward: 304.12 | Patience: 5/10\n",
            "Update performed at episode 1535\n",
            "Update 47 | Avg Reward: 289.23 | Patience: 6/10\n",
            "Update performed at episode 1567\n",
            "Update 48 | Avg Reward: 316.74 | Patience: 7/10\n",
            "Update performed at episode 1599\n",
            "Update 49 | New Best: 356.84 | Model Saved.\n",
            "Update performed at episode 1631\n",
            "Update 50 | Avg Reward: 336.02 | Patience: 1/10\n",
            "Update performed at episode 1663\n",
            "Update 51 | New Best: 378.09 | Model Saved.\n",
            "Update performed at episode 1695\n",
            "Update 52 | Avg Reward: 350.93 | Patience: 1/10\n",
            "Update performed at episode 1727\n",
            "Update 53 | Avg Reward: 361.30 | Patience: 2/10\n",
            "Update performed at episode 1759\n",
            "Update 54 | Avg Reward: 340.28 | Patience: 3/10\n",
            "Update performed at episode 1791\n",
            "Update 55 | Avg Reward: 340.83 | Patience: 4/10\n",
            "Update performed at episode 1823\n",
            "Update 56 | Avg Reward: 341.57 | Patience: 5/10\n",
            "Update performed at episode 1855\n",
            "Update 57 | Avg Reward: 338.47 | Patience: 6/10\n",
            "Update performed at episode 1887\n",
            "Update 58 | Avg Reward: 366.42 | Patience: 7/10\n",
            "Update performed at episode 1919\n",
            "Update 59 | Avg Reward: 322.30 | Patience: 8/10\n",
            "Update performed at episode 1951\n",
            "Update 60 | Avg Reward: 338.37 | Patience: 9/10\n",
            "Update performed at episode 1983\n",
            "Update 61 | Avg Reward: 356.25 | Patience: 10/10\n",
            "Early stopping triggered at episode 1983\n",
            "Training Complete. Best average reward: 378.09\n"
          ]
        }
      ],
      "source": [
        "# Training Helpers\n",
        "best_avg_reward = -float('inf')     # best avarage reward \n",
        "patience_counter = 0                # counter of the no improve updates\n",
        "output_model_path = output_dir + '/policy_model.ph'\n",
        "\n",
        "print(\"Starting Training...\")\n",
        "\n",
        "# Run all the episodes\n",
        "for episode in range(MAX_EPISODES):\n",
        "    # 1. SETUP EPISODE\n",
        "    # Each episode is a different enviroment configuration\n",
        "    seed = sample_seed()\n",
        "    obs, info = env.reset(seed)\n",
        "\n",
        "\n",
        "    # Plan path using a sampled radius\n",
        "    radius = sample_radius()\n",
        "    planner = DubinsPlanner(curvature=radius, step_size=0.2)\n",
        "    path = planner.get_path(env.state, env.goal)\n",
        "\n",
        "    if path is None: \n",
        "        continue\n",
        "\n",
        "    ep_reward, path_idx = 0, 0\n",
        "    episode_errors = []\n",
        "\n",
        "\n",
        "    # 2. INTERACTION LOOP (THE EPISODE)\n",
        "    for t in range(MAX_STEPS):\n",
        "        # A. Get observation relative to current target waypoint\n",
        "        # Input of the network is the Ego-Centric Observation Vector: \n",
        "        #   1. rho (Ï): [0, inf]        Euclidean distance to the goal.        \n",
        "        #   2. alpha (Î±): [-pi, pi]     The angle of the goal *relative* to the robot's current heading.\n",
        "        #   3. d_theta (Î´Î¸): [-pi, pi]  The difference between the desired goal orientation and current heading.\n",
        "        target_waypoint = path[path_idx]        # The goal is to reach the next waypoint of the path\n",
        "        tracking_obs = get_tracking_obs(target_waypoint, env.state)       \n",
        "        \n",
        "        # B. Agent chooses action and records log_prob/obs in memory\n",
        "        action = get_action(tracking_obs)\n",
        "\n",
        "        # C. Environment Step\n",
        "        _, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        # D. Update Path Tracking (Find closest waypoint and move target forward)\n",
        "        error, path_idx, checkpoints_cleared = get_path_tracking_info(path, env.state[:2], path_idx)\n",
        "        episode_errors.append(error)\n",
        "        \n",
        "        # E. Determine Terminal State and Reason\n",
        "        terminal_reason = None\n",
        "        if info['is_success']: terminal_reason = \"goal\"\n",
        "        elif info['collision']: terminal_reason = \"collision\"\n",
        "        elif error > 2.0:       terminal_reason = \"off_path\"\n",
        "        elif t >= MAX_STEPS - 1: terminal_reason = \"timeout\"\n",
        "        \n",
        "        # F. Reward Calculation\n",
        "        # Pass the tracking observation (rho, alpha, d_theta), action, and status\n",
        "        step_reward = reward_calculator.compute_reward(\n",
        "            tracking_obs=tracking_obs,\n",
        "            action=action,\n",
        "            checkpoints_cleared=checkpoints_cleared,\n",
        "            terminal_reason=terminal_reason\n",
        "        )\n",
        "\n",
        "        buffer.store_reward(step_reward)\n",
        "        ep_reward += step_reward\n",
        "\n",
        "        if terminal_reason:\n",
        "            break\n",
        "\n",
        "\n",
        "    # 3. EPISODE END: Process Return-to-Go\n",
        "    buffer.finish_episode(GAMMA)\n",
        "    buffer.store_metrics(episode_reward=ep_reward)\n",
        "\n",
        "\n",
        "    # 4. BATCH UPDATE: Perform Learning\n",
        "    if (episode + 1) % BATCH_SIZE == 0:\n",
        "        p_loss = update() \n",
        "        print(f\"Update performed at episode {episode}\")\n",
        "\n",
        "        # Calculate performance over the last batch\n",
        "        avg_r = np.mean(buffer.episode_reward_history[-BATCH_SIZE:]) # Average reward of the last batch\n",
        "        buffer.store_metrics(batch_reward=avg_r)\n",
        "        \n",
        "        \n",
        "        # 5. EARLY STOPPING & SAVING\n",
        "        if avg_r > best_avg_reward:\n",
        "            best_avg_reward = avg_r\n",
        "            patience_counter = 0\n",
        "            # Store the best version of the weights \n",
        "            torch.save(policy.state_dict(), output_model_path)\n",
        "            print(f\"Update {episode//BATCH_SIZE} | New Best: {best_avg_reward:.2f} | Model Saved.\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"Update {episode//BATCH_SIZE} | Avg Reward: {avg_r:.2f} | Patience: {patience_counter}/{PATIENCE}\")\n",
        "\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(f\"Early stopping triggered at episode {episode}\")\n",
        "            break\n",
        "\n",
        "\n",
        "print(f\"Training Complete. Best average reward: {best_avg_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "534aaeb7",
      "metadata": {
        "id": "534aaeb7"
      },
      "source": [
        "### Cell 9: Visualization & Save\n",
        "Plot the learning curve to verify convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c059685",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "6c059685",
        "outputId": "2d116786-5a07-406b-d7b5-8086aa59ac57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Execution complete.\n",
            "Directory: experiments/run_20260109_100247\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def ensure_numpy(data):\n",
        "    \"\"\"Helper to convert lists of floats or tensors to a clean numpy array.\"\"\"\n",
        "    return np.array([d.detach().cpu().item() if torch.is_tensor(d) else d for d in data])\n",
        "\n",
        "# 1. Prepare Data\n",
        "ep_rewards = ensure_numpy(buffer.episode_reward_history)\n",
        "batch_rewards = ensure_numpy(buffer.batch_reward_history)\n",
        "p_losses = ensure_numpy(buffer.policy_loss_history)\n",
        "\n",
        "\n",
        "# 2. Setup Figure\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "plt.suptitle(f\"REINFORCE Training Metrics\", fontsize=16)\n",
        "\n",
        "# Plot 1: Total Reward per Episode\n",
        "axes[0].plot(ep_rewards, color='blue', alpha=0.3, label='Episode Reward')\n",
        "if len(batch_rewards) > 0:\n",
        "    x_batch = (np.arange(len(batch_rewards)) + 1) * BATCH_SIZE - 1\n",
        "    axes[0].plot(x_batch, batch_rewards, color='green', marker='o', \n",
        "                    markersize=4, linewidth=2, label='Batch Average')\n",
        "axes[0].set_title(\"Reward Progress\")\n",
        "axes[0].set_xlabel(\"Episode\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Plot 2: Policy Loss\n",
        "axes[1].plot(p_losses, color='red')\n",
        "axes[1].set_title(\"Policy Loss\")\n",
        "axes[1].set_xlabel(\"Update Batch\")\n",
        "axes[1].grid(True)\n",
        "\n",
        "# 3. Save and Close\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plot_path = os.path.join(output_dir, \"training_metrics.png\")\n",
        "plt.savefig(plot_path)\n",
        "plt.close() # Prevents duplicate display in notebooks\n",
        "\n",
        "\n",
        "# 4. SAVING DATA \n",
        "# Saves history arrays to disk for future analysis.\n",
        "data_to_save = {\n",
        "    \"episode_rewards\": ep_rewards,\n",
        "    \"policy_losses\": p_losses,\n",
        "}\n",
        "\n",
        "for name, arr in data_to_save.items():\n",
        "    np.save(os.path.join(output_dir, f\"{name}.npy\"), arr)\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"Execution complete.\")\n",
        "print(f\"Directory: {output_dir}\")\n",
        "print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e999fc5a",
      "metadata": {},
      "source": [
        "### Cell 10: Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0ddbc29",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--> Loading Model: v1_no_baseline/policy_model.ph\n",
            "RL Controller loaded successfully on cuda\n",
            "Starting 50 episodes...\n",
            "Progress: 50/50\n",
            "Evaluation Complete.\n",
            "\n",
            "============================================================\n",
            " EVALUATION REPORT: last_run\n",
            "============================================================\n",
            "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
            "â”‚ Metric       â”‚ Value        â”‚ Metric        â”‚ Value             â”‚ Metric       â”‚ Value           â”‚\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ Status       â”‚ âœ… EXCELLENT â”‚ Mean CTE      â”‚ 0.0103 Â± 0.0015 m â”‚ Avg Velocity â”‚ 0.95 Â± 0.01 m/s â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Success Rate â”‚ 100.0 %      â”‚ Max CTE (Avg) â”‚ 0.0282 m          â”‚ Smoothness   â”‚ 0.0559 Â± 0.0092 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Steps (Avg)  â”‚ 295.1 Â± 26.8 â”‚ Tortuosity    â”‚ 1.000 Â± 0.001     â”‚ Avg Energy   â”‚ 15.15 Â± 1.34    â”‚\n",
            "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n",
            "\n",
            "Report saved to text file: v1_no_baseline/evaluation_report.txt\n"
          ]
        }
      ],
      "source": [
        "from evaluation.controllers_evaluation import evaluate_single_model\n",
        "\n",
        "report_path = output_dir + '/evaluation_report.txt'\n",
        "evaluate_single_model(output_model_path, \"last_run\", 50, 0, \n",
        "                      verbose=True, save_csv=False, txt_file_path=report_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WrZfIDTFYCTZ",
      "metadata": {
        "id": "WrZfIDTFYCTZ"
      },
      "source": [
        "### Cell 11: âš  Delete the train directory ( only for bed results )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3UCvSptGYWHD",
      "metadata": {
        "id": "3UCvSptGYWHD"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# --- DELETE CURRENT EXPERIMENT DIRECTORY ---\n",
        "# Ask for confirmation to prevent accidental deletion\n",
        "confirm = input(f\"Do you want to delete the directory '{run_id}'? (y/n): \")\n",
        "\n",
        "if confirm.lower() == 'y':\n",
        "    try:\n",
        "        # shutil.rmtree deletes a directory and all its contents\n",
        "        shutil.rmtree(output_dir)\n",
        "        print(f\"Successfully deleted: {output_dir}\")\n",
        "    except OSError as e:\n",
        "        print(f\"Error: {e.strerror}. Could not delete the directory.\")\n",
        "else:\n",
        "    print(\"Deletion cancelled. Experiment results preserved.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
