{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "nhpYDINPRKzp",
      "metadata": {
        "id": "nhpYDINPRKzp"
      },
      "source": [
        "### Cell 0: ( Only for Colab ) Install the git **repository**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cMzorY91RWhY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMzorY91RWhY",
        "outputId": "cd4777f2-535a-470f-9437-c33d6319bbc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'final_assignment'...\n",
            "remote: Enumerating objects: 116, done.\u001b[K\n",
            "remote: Counting objects: 100% (116/116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 116 (delta 37), reused 105 (delta 32), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (116/116), 501.87 KiB | 2.50 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# 1. Clone the repository\n",
        "!git clone -b trajectory_tracking_policy https://github.com/orc-podavini-grisenti/final_assignment\n",
        "\n",
        "# 2. Point the project_root specifically to the cloned folder\n",
        "# In Colab, the folder structure will be /content/final_assignment/\n",
        "project_root = os.path.abspath('/content/final_assignment')\n",
        "\n",
        "# 3. Add that specific folder to the system path\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "\n",
        "\n",
        "# 4. Link the Drive to the colab in order to store the results\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "SAVE_ON_DRIVE = True\n",
        "drive_path = \"/content/drive/MyDrive/final_assignment/models_saved\"\n",
        "\n",
        "# Crea la cartella se non esiste\n",
        "if not os.path.exists(drive_path):\n",
        "    os.makedirs(drive_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12184240",
      "metadata": {
        "id": "12184240"
      },
      "source": [
        "### Cell 1: Imports & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "efb0bce8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efb0bce8",
        "outputId": "d321196c-079f-4e10-e41a-3e5e006c3dae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on: cuda\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- PATH SETUP ---\n",
        "# Add the project root to sys.path so we can import envs/planner\n",
        "# Adjust '..' depending on where this notebook is located relative to root\n",
        "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "\n",
        "from envs.unicycle_env import UnicycleEnv\n",
        "from planner.dubins_planner import DubinsPlanner\n",
        "from models.trajectory_tracking_network import TTNetwork\n",
        "from utils.normalization import ObservationNormalizer\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Running on: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9f474e8",
      "metadata": {
        "id": "f9f474e8"
      },
      "source": [
        "### Cell 2: Load Training Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "726a095a",
      "metadata": {
        "id": "726a095a"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "MAX_EPISODES = 100\n",
        "MAX_STEPS    = 20\n",
        "LR           = 3e-4\n",
        "BATCH_SIZE   = 2\n",
        "GAMMA        = 0.99"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "655cb984",
      "metadata": {
        "id": "655cb984"
      },
      "source": [
        "### Cell 3: Intialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7c4967fd",
      "metadata": {
        "id": "7c4967fd"
      },
      "outputs": [],
      "source": [
        "# Init Enviroment and Dubins Planner\n",
        "env = UnicycleEnv()\n",
        "planner = DubinsPlanner(curvature_max=1.5, step_size=0.2)\n",
        "\n",
        "\n",
        "# Initilize the Network.\n",
        "obs_dim = 3     # Obs dim is 3 (rho, alpha, d_theta)\n",
        "action_dim = 2  # v, omega\n",
        "\n",
        "policy = TTNetwork(obs_dim, action_dim).to(device)\n",
        "\n",
        "# Initializa the otpimizar \n",
        "optimizer = optim.Adam(policy.parameters(), lr=LR)\n",
        "\n",
        "# Initializa the input normalizer\n",
        "obs_normalizer = ObservationNormalizer(max_dist=3.0)\n",
        "\n",
        "gamma = GAMMA\n",
        "\n",
        "# Memory buffer for the current episode\n",
        "log_probs = []\n",
        "rewards = []\n",
        "\n",
        "# Memory buffer fot the current batch\n",
        "loss_history = [] # Store all the batch losses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a445984",
      "metadata": {
        "id": "5a445984"
      },
      "source": [
        "### Cell 4: Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "86545a72",
      "metadata": {
        "id": "86545a72"
      },
      "outputs": [],
      "source": [
        "def get_action(obs):\n",
        "    \"\"\"Samples an action and stores its log_probability.\"\"\"\n",
        "    # Use the imported normalizer to normaliza the inputs features to give the same relevance\n",
        "    norm_obs = obs_normalizer.normalize_tt(obs)\n",
        "\n",
        "    obs_t = torch.FloatTensor(norm_obs).unsqueeze(0).to(device)\n",
        "\n",
        "    # Call the network to get a action distribution\n",
        "    mean, std = policy(obs_t)\n",
        "    dist = Normal(mean, std)\n",
        "\n",
        "    # Sample a randome action from this distribution\n",
        "    # NB: randomcicity ensure exploration\n",
        "    action = dist.sample()\n",
        "\n",
        "    # Store log_prob for the update step later\n",
        "    # Sum over action dimensions (v and omega)\n",
        "    log_prob = dist.log_prob(action).sum(dim=1)\n",
        "    log_probs.append(log_prob)\n",
        "\n",
        "    # Return numpy action for the environment\n",
        "    # Clip to ensure physical validity [-1, 1]\n",
        "    return np.clip(action.cpu().detach().numpy()[0], -1.0, 1.0)\n",
        "\n",
        "\n",
        "def store_reward(reward):\n",
        "    rewards.append(reward)\n",
        "\n",
        "def store_batch_loss(loss):\n",
        "    loss_history.append(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5a65dfe",
      "metadata": {
        "id": "d5a65dfe"
      },
      "source": [
        "### Cell 5: Reinforce Update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b58828e9",
      "metadata": {
        "id": "b58828e9"
      },
      "outputs": [],
      "source": [
        "# Global storage for the whole batch of trajectories\n",
        "batch_log_probs = []\n",
        "batch_returns = []\n",
        "\n",
        "def calculate_discounted_returns():\n",
        "    \"\"\"\n",
        "    Calculates the 'Return-to-Go' (G_t) for the current episode using causality.\n",
        "\n",
        "    Mathematical Concept:\n",
        "    Instead of using the total reward C(tau) for every step, we use G_t:\n",
        "    G_t = sum_{k=t}^H gamma^(k-t) * r_k\n",
        "\n",
        "    This reduces variance because actions at time 't' are only reinforced\n",
        "    based on rewards that happen *after* time 't' (Causality).\n",
        "    \"\"\"\n",
        "    global log_probs, rewards, batch_log_probs, batch_returns\n",
        "\n",
        "    G_t = 0\n",
        "    returns = []\n",
        "\n",
        "    # 1. Calculate Discounted Returns (Backwards)\n",
        "    # We iterate backwards to easily compute G_t = r_t + gamma * G_{t+1}\n",
        "    for r in reversed(rewards):\n",
        "        G_t = r + gamma * G_t\n",
        "        returns.insert(0, G_t)\n",
        "\n",
        "    # 2. Store for the Batch Update\n",
        "    # We do NOT normalize here. We wait to collect all episodes in the batch\n",
        "    # so we can normalize across the entire dataset (Batch-wise normalization).\n",
        "    batch_log_probs.extend(log_probs)\n",
        "    batch_returns.extend(returns)\n",
        "\n",
        "    # Clear episode memory\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "\n",
        "def update():\n",
        "    \"\"\"\n",
        "    Performs the Policy Gradient Update using the REINFORCE algorithm.\n",
        "\n",
        "    Equation Implemented:\n",
        "    ∇J ≈ (1/N) * Σ_i Σ_t [ ∇ log π(u_t|x_t) * G_t ]\n",
        "\n",
        "    Where:\n",
        "    - (1/N): Averaging over the batch size (len(batch_returns))\n",
        "    - Σ_t: Summing over time steps (done via torch.sum or iterative addition)\n",
        "    - ∇ log π: Gradients calculated via loss.backward() on log_probs\n",
        "    - G_t: The normalized returns-to-go\n",
        "    \"\"\"\n",
        "    global batch_log_probs, batch_returns, optimizer\n",
        "\n",
        "    # Security check: if batch is empty, skip\n",
        "    if len(batch_returns) == 0:\n",
        "        return\n",
        "\n",
        "    # 1. Prepare Data\n",
        "    # Convert list of returns to a Tensor\n",
        "    returns_t = torch.tensor(batch_returns).float().to(device)\n",
        "\n",
        "    # 2. Batch-Wise Normalization (Crucial for Stability)\n",
        "    # This aligns the returns to be roughly N(0, 1), stabilizing the gradients.\n",
        "    # It acts as a baseline: actions with G_t > mean are encouraged (+),\n",
        "    # actions with G_t < mean are discouraged (-).\n",
        "    if len(returns_t) > 1:\n",
        "        returns_t = (returns_t - returns_t.mean()) / (returns_t.std() + 1e-9)\n",
        "\n",
        "    # 3. Calculate Policy Loss\n",
        "    # We want to maximize J, so we minimize Loss = -J\n",
        "    # Loss = - Σ ( log_prob * G_t )\n",
        "    policy_loss = []\n",
        "    for log_prob, G_t in zip(batch_log_probs, returns_t):\n",
        "        # Term: - log π(u_t|x_t) * G_t\n",
        "        policy_loss.append(-log_prob * G_t)\n",
        "\n",
        "    # Summing all terms corresponds to the Σ_i Σ_t part of the equation\n",
        "    loss = torch.stack(policy_loss).sum()\n",
        "\n",
        "    # 4. Backpropagation\n",
        "    # Dividing by the number of episodes corresponds to the (1/N) term\n",
        "    # This keeps the gradient magnitude consistent regardless of batch size.\n",
        "    loss = loss / BATCH_SIZE\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()  # This computes ∇_theta\n",
        "\n",
        "    # Gradient Clipping (Optional but recommended to prevent exploding gradients)\n",
        "    torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n",
        "\n",
        "    optimizer.step() # Update parameters theta\n",
        "\n",
        "    # 5. Logging and Cleanup\n",
        "    store_batch_loss(loss.item())\n",
        "\n",
        "    # Clear batch memory for the next set of episodes\n",
        "    batch_log_probs = []\n",
        "    batch_returns = []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07f76da5",
      "metadata": {
        "id": "07f76da5"
      },
      "source": [
        "### Cell 6: Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "196fb4c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "196fb4c8",
        "outputId": "4bc0af40-ff5d-449b-d559-c5afc2a0aba9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Training...\n",
            "Update performed at episode 1\n",
            "Episode 1 | Avg Reward: -111.3 | Path Len: 16, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 3\n",
            "Episode 3 | Avg Reward: -108.4 | Path Len: 10, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 5\n",
            "Episode 5 | Avg Reward: -109.2 | Path Len: 24, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 7\n",
            "Episode 7 | Avg Reward: -113.3 | Path Len: 16, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 9\n",
            "Episode 9 | Avg Reward: -118.3 | Path Len: 22, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 11\n",
            "Episode 11 | Avg Reward: -116.3 | Path Len: 17, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 13\n",
            "Episode 13 | Avg Reward: -114.3 | Path Len: 18, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 15\n",
            "Episode 15 | Avg Reward: -114.6 | Path Len: 14, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 17\n",
            "Episode 17 | Avg Reward: -114.8 | Path Len: 15, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 19\n",
            "Episode 19 | Avg Reward: -113.9 | Path Len: 22, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 21\n",
            "Episode 21 | Avg Reward: -110.6 | Path Len: 9, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 23\n",
            "Episode 23 | Avg Reward: -105.6 | Path Len: 12, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 25\n",
            "Episode 25 | Avg Reward: -107.8 | Path Len: 26, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 27\n",
            "Episode 27 | Avg Reward: -108.2 | Path Len: 23, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 29\n",
            "Episode 29 | Avg Reward: -102.3 | Path Len: 13, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 31\n",
            "Episode 31 | Avg Reward: -102.0 | Path Len: 22, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 33\n",
            "Episode 33 | Avg Reward: -102.3 | Path Len: 17, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 35\n",
            "Episode 35 | Avg Reward: -99.6 | Path Len: 26, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 37\n",
            "Episode 37 | Avg Reward: -101.7 | Path Len: 15, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 39\n",
            "Episode 39 | Avg Reward: -102.2 | Path Len: 12, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 41\n",
            "Episode 41 | Avg Reward: -105.5 | Path Len: 18, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 43\n",
            "Episode 43 | Avg Reward: -113.9 | Path Len: 23, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 45\n",
            "Episode 45 | Avg Reward: -109.5 | Path Len: 23, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 47\n",
            "Episode 47 | Avg Reward: -107.4 | Path Len: 17, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 49\n",
            "Episode 49 | Avg Reward: -111.5 | Path Len: 17, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 51\n",
            "Episode 51 | Avg Reward: -113.9 | Path Len: 22, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 53\n",
            "Episode 53 | Avg Reward: -113.9 | Path Len: 17, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 55\n",
            "Episode 55 | Avg Reward: -116.3 | Path Len: 26, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 57\n",
            "Episode 57 | Avg Reward: -116.4 | Path Len: 26, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 59\n",
            "Episode 59 | Avg Reward: -116.2 | Path Len: 23, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 61\n",
            "Episode 61 | Avg Reward: -113.6 | Path Len: 16, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 63\n",
            "Episode 63 | Avg Reward: -107.3 | Path Len: 10, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 65\n",
            "Episode 65 | Avg Reward: -110.0 | Path Len: 20, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 67\n",
            "Episode 67 | Avg Reward: -107.6 | Path Len: 9, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 69\n",
            "Episode 69 | Avg Reward: -104.8 | Path Len: 14, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 71\n",
            "Episode 71 | Avg Reward: -101.3 | Path Len: 23, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 73\n",
            "Episode 73 | Avg Reward: -100.2 | Path Len: 26, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 75\n",
            "Episode 75 | Avg Reward: -100.8 | Path Len: 21, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 77\n",
            "Episode 77 | Avg Reward: -98.6 | Path Len: 12, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 79\n",
            "Episode 79 | Avg Reward: -95.2 | Path Len: 14, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 81\n",
            "Episode 81 | Avg Reward: -98.8 | Path Len: 17, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 83\n",
            "Episode 83 | Avg Reward: -105.0 | Path Len: 21, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 85\n",
            "Episode 85 | Avg Reward: -104.3 | Path Len: 13, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 87\n",
            "Episode 87 | Avg Reward: -106.6 | Path Len: 15, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 89\n",
            "Episode 89 | Avg Reward: -109.7 | Path Len: 18, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 91\n",
            "Episode 91 | Avg Reward: -113.8 | Path Len: 26, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 93\n",
            "Episode 93 | Avg Reward: -113.5 | Path Len: 14, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 95\n",
            "Episode 95 | Avg Reward: -112.0 | Path Len: 17, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 97\n",
            "Episode 97 | Avg Reward: -107.3 | Path Len: 29, Path Step Reached: 0, Max step reached\n",
            "Update performed at episode 99\n",
            "Episode 99 | Avg Reward: -110.3 | Path Len: 15, Path Step Reached: 0, Max step reached\n",
            "Training Complete.\n"
          ]
        }
      ],
      "source": [
        "reward_history = []\n",
        "\n",
        "print(\"Starting Training...\")\n",
        "\n",
        "for episode in range(MAX_EPISODES):\n",
        "    # 1. Reset & Plan\n",
        "    obs, info = env.reset()\n",
        "    start_pose = env.state\n",
        "    goal_pose = env.goal\n",
        "\n",
        "    # Generate Reference Trajectory\n",
        "    path = planner.get_path(start_pose, goal_pose)\n",
        "    if path is None: continue # Skip hard planning cases\n",
        "\n",
        "    path_idx = 0\n",
        "    ep_reward = 0\n",
        "\n",
        "    breack_reason = \"Max step reached\"      # reason of the episod termination\n",
        "\n",
        "    # 2. Episode Rollout\n",
        "    for t in range(MAX_STEPS):\n",
        "        # A. Determine Target\n",
        "        target = path[path_idx]\n",
        "\n",
        "        # B. Observation (Relative to Waypoint)\n",
        "        # Extract only the first 3 element: [rho, alpha, d_theta]\n",
        "        # Discard the obstacle part, in the first section are not relevant\n",
        "        tracking_obs = obs[:3]\n",
        "\n",
        "        # C. Action\n",
        "        action = get_action(tracking_obs)\n",
        "\n",
        "        # D. Step\n",
        "        # We ignore env reward, we calculate our own\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        # E. Reward\n",
        "        store_reward(reward)\n",
        "        ep_reward += reward\n",
        "\n",
        "        # F. Update Waypoint (Pure Pursuit Logic)\n",
        "        # If close to current waypoint, target the next one\n",
        "        if tracking_obs[0] < 0.1 and path_idx < len(path) - 1:\n",
        "            path_idx += 1\n",
        "\n",
        "        # G. Termination\n",
        "        dist_to_final = np.linalg.norm(env.state[:2] - goal_pose[:2])\n",
        "        if dist_to_final < 0.1 and path_idx == len(path) - 1:\n",
        "            breack_reason = \"Goal Reached\"\n",
        "            break\n",
        "        if info['collision']:\n",
        "            breack_reason = \"Collision\"\n",
        "            break\n",
        "\n",
        "    # 3. Compute the Returns to Go\n",
        "    calculate_discounted_returns()\n",
        "\n",
        "    # 4. If we have collected 'm' trajectories, perform the update\n",
        "    if (episode + 1) % BATCH_SIZE == 0:\n",
        "        update() # This effectively divides by m\n",
        "        print(f\"Update performed at episode {episode}\")\n",
        "\n",
        "    reward_history.append(ep_reward)\n",
        "\n",
        "    if (episode + 1) % BATCH_SIZE == 0:\n",
        "        avg_r = np.mean(reward_history[-20:])\n",
        "        print(f\"Episode {episode} | Avg Reward: {avg_r:.1f} | Path Len: {len(path)}, Path Step Reached: {path_idx}, {breack_reason}\")\n",
        "\n",
        "print(\"Training Complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "534aaeb7",
      "metadata": {
        "id": "534aaeb7"
      },
      "source": [
        "### Cell 7: Visualization & Save\n",
        "Plot the learning curve to verify convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6c059685",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "6c059685",
        "outputId": "12a5fee8-ba4b-4bd3-8c13-5345364198b5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'SAVE_ON_DRIVE' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m current_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m run_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mSAVE_ON_DRIVE\u001b[49m:\n\u001b[1;32m     12\u001b[0m     base_output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(drive_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiments\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SAVE_ON_DRIVE' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "# --- 1. SETUP DIRECTORY STRUCTURE ---\n",
        "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "run_id = f\"run_{current_time}\"\n",
        "\n",
        "if SAVE_ON_DRIVE:\n",
        "    base_output_path = os.path.join(drive_path, \"experiments\")\n",
        "else:\n",
        "    base_output_path = \"experiments\"\n",
        "\n",
        "output_dir = os.path.join(base_output_path, run_id)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Results will be saved to: {output_dir}\")\n",
        "\n",
        "# --- 2. PLOTTING AND SAVING FIGURES ---\n",
        "# We create a figure with 2 subplots (1 row, 2 columns)\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Process histories to ensure they are plottable (handle tensors)\n",
        "clean_rewards = [r.detach().cpu().item() if torch.is_tensor(r) else r for r in reward_history]\n",
        "clean_losses = [l.detach().cpu().item() if torch.is_tensor(l) else l for l in loss_history]\n",
        "\n",
        "# Plot 1: Total Reward\n",
        "ax1.plot(clean_rewards, color='blue')\n",
        "ax1.set_title(\"Training Reward\")\n",
        "ax1.set_xlabel(\"Episode\")\n",
        "ax1.set_ylabel(\"Total Reward\")\n",
        "ax1.grid(True)\n",
        "\n",
        "# Plot 2: Training Loss\n",
        "# Note: Ensure you are appending your loss values to a list named 'loss_history' during training\n",
        "ax2.plot(clean_losses, color='red')\n",
        "ax2.set_title(\"Training Loss\")\n",
        "ax2.set_xlabel(\"Episode\")\n",
        "ax2.set_ylabel(\"Loss\")\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.suptitle(f\"REINFORCE Training Metrics - {run_id}\")\n",
        "plt.tight_layout() # Adjust spacing so titles don't overlap\n",
        "\n",
        "# Save the combined plot\n",
        "plot_filename = \"training_metrics_plot.png\"\n",
        "plt.savefig(os.path.join(output_dir, plot_filename))\n",
        "plt.show()\n",
        "\n",
        "# --- 3. SAVING MODEL AND DATA ---\n",
        "model_filename = \"policy_model.pth\"\n",
        "torch.save(policy.state_dict(), os.path.join(output_dir, model_filename))\n",
        "\n",
        "# Save both histories for future analysis\n",
        "np.save(os.path.join(output_dir, \"reward_history.npy\"), np.array(clean_rewards))\n",
        "np.save(os.path.join(output_dir, \"loss_history.npy\"), np.array(clean_losses))\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"Execution complete.\")\n",
        "print(f\"Directory: {output_dir}\")\n",
        "print(f\"Saved: {model_filename}, {plot_filename}, histories (.npy)\")\n",
        "print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WrZfIDTFYCTZ",
      "metadata": {
        "id": "WrZfIDTFYCTZ"
      },
      "source": [
        "### Cell 8: ⚠ Delete the train directory ( only for bed results )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3UCvSptGYWHD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UCvSptGYWHD",
        "outputId": "81527fb0-eac3-4e21-a14e-7cde7501f65a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Do you want to delete the directory 'run_20260101_175404'? (y/n): y\n",
            "Successfully deleted: /content/drive/MyDrive/final_assignment/models_saved/experiments/run_20260101_175404\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "# --- DELETE CURRENT EXPERIMENT DIRECTORY ---\n",
        "# Ask for confirmation to prevent accidental deletion\n",
        "confirm = input(f\"Do you want to delete the directory '{run_id}'? (y/n): \")\n",
        "\n",
        "if confirm.lower() == 'y':\n",
        "    try:\n",
        "        # shutil.rmtree deletes a directory and all its contents\n",
        "        shutil.rmtree(output_dir)\n",
        "        print(f\"Successfully deleted: {output_dir}\")\n",
        "    except OSError as e:\n",
        "        print(f\"Error: {e.strerror}. Could not delete the directory.\")\n",
        "else:\n",
        "    print(\"Deletion cancelled. Experiment results preserved.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
